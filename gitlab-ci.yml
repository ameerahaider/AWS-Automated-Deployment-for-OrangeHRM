image: docker:latest

services:
  - docker:dind

stages:
  - build
  - upload
  - deploy

variables:
  ECR_REPO_URL: 'public.ecr.aws/f0l3p7l5/orangehrm'
  TF_ROOT: ${CI_PROJECT_DIR}
  TF_ADDRESS: ${CI_API_V4_URL}/projects/${CI_PROJECT_ID}/terraform/state/terraform
  TERRAFORM_VERSION: '0.15.5'
  AWS_ROLE_ARN: arn:aws:iam::425342967151:role/CICD_Role_AllowAll
  AWS_WEB_IDENTITY_TOKEN_FILE: /tmp/web-identity-token

.authenticate: &authenticate
    - echo "$CI_JOB_JWT_V2" > $AWS_WEB_IDENTITY_TOKEN_FILE

before_script:
  - apk add --no-cache curl py-pip python3 groff git groff openssh-client curl unzip bash jq
  - python3 -m pip install --upgrade pip
  - python3 -m ensurepip
  - python3 -m pip install --upgrade awscli

  - curl -LO "https://releases.hashicorp.com/terraform/${TERRAFORM_VERSION}/terraform_${TERRAFORM_VERSION}_linux_amd64.zip"
  - unzip "terraform_${TERRAFORM_VERSION}_linux_amd64.zip" -d /usr/local/bin/
  - chmod +x /usr/local/bin/terraform

BuildInfra:
  stage: build
  only:
    - infra
  script:
  - *authenticate
  - |
    # Use the image ID within your Terraform module
    terraform init -backend-config="bucket=my-terraform-state-bucket-cicd" -backend-config="key=terraform/state/terraform.tfstate" -backend-config="region=us-east-1" -backend-config="encrypt=true"      
    terraform plan -out "planfile"
    terraform apply -auto-approve -input=false "planfile"

PushImage:
  stage: upload
  only:
    - deploy
  script:
    - *authenticate
      # Fetch the AWS account ID and default region dynamically
    - AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query "Account" --output text)

    - AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query "Account" --output text)
    - aws ecr-public get-login-password --region us-east-1 | docker login --username AWS --password-stdin public.ecr.aws/$AWS_ACCOUNT_ID
    - docker build -t $ECR_REGISTRY/$ECR_REPOSITORY:$CI_COMMIT_SHA .
    - docker push $ECR_REGISTRY/$ECR_REPOSITORY:$CI_COMMIT_SHA

DeployApp:
  stage: deploy
  only:
    - deploy
  script:  
    - *authenticate
    - |
      # Fetch the AWS account ID and default region dynamicall
      AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query "Account" --output text)

      # Getting IPs and parsing to a shell array
      PRIVATE_EC2_IPS=$(aws ssm get-parameter --name "private-ips" --output text --query "Parameter.Value" | jq -r '.[]')
      JUMP_SERVER_IP=$(aws ssm get-parameter --name "jump_server-ip" --output text --query "Parameter.Value")
      SSH_PRIVATE_KEY=$(aws ssm get-parameter --name "priv_key" --output text --query "Parameter.Value")
      SSH_PUBLIC_KEY=$(aws ssm get-parameter --name "pub_key" --output text --query "Parameter.Value")

      # Print for debugging
      echo "Private IPs: $PRIVATE_EC2_IPS"
      echo "Jump server IP: $JUMP_SERVER_IP"
      echo "Private Key: $SSH_PRIVATE_KEY"
      echo "Public Key: $SSH_PUBLIC_KEY"

      # Ensure that the ~/.ssh/ directory exists
      mkdir -p ~/.ssh/
      chmod 700 ~/.ssh/

      # Use the SSH private key directly in the SSH commands
      echo -e "$SSH_PRIVATE_KEY" | tr -d '\r' > ~/.ssh/id_rsa
      chmod 600 ~/.ssh/id_rsa
      eval $(ssh-agent)
      ssh-add ~/.ssh/id_rsa

      # Write the public key to a temporary file
      echo -e "$SSH_PUBLIC_KEY" > ~/.ssh/id_rsa.pub

      # Copy SSH_PRIVATE_KEY to the Jump Server's /home/ec2-user directory
      scp -o StrictHostKeyChecking=no -i ~/.ssh/id_rsa ~/.ssh/id_rsa ec2-user@$JUMP_SERVER_IP:/home/ec2-user/.ssh/id_rsa
      ssh -o StrictHostKeyChecking=no -i ~/.ssh/id_rsa ec2-user@$JUMP_SERVER_IP "chmod 600 /home/ec2-user/.ssh/id_rsa"

      for IP in $PRIVATE_EC2_IPS; do

          # SSH into the jump server, and from there SSH into the target EC2 instance
          ssh -o StrictHostKeyChecking=no -i ~/.ssh/id_rsa ec2-user@$JUMP_SERVER_IP "ssh -o StrictHostKeyChecking=no -i /home/ec2-user/.ssh/id_rsa ec2-user@$IP 'sudo systemctl stop httpd'"
          
          # Stop and remove containers only if there are running containers
          RUNNING_CONTAINERS=$(ssh -o StrictHostKeyChecking=no -i ~/.ssh/id_rsa ec2-user@$JUMP_SERVER_IP "ssh -o StrictHostKeyChecking=no -i /home/ec2-user/.ssh/id_rsa ec2-user@$IP 'sudo docker ps -q'")
          
          if [ -n "$RUNNING_CONTAINERS" ]; then
              ssh -o StrictHostKeyChecking=no -i ~/.ssh/id_rsa ec2-user@$JUMP_SERVER_IP "ssh -o StrictHostKeyChecking=no -i /home/ec2-user/.ssh/id_rsa ec2-user@$IP 'sudo docker stop $RUNNING_CONTAINERS'"
              ssh -o StrictHostKeyChecking=no -i ~/.ssh/id_rsa ec2-user@$JUMP_SERVER_IP "ssh -o StrictHostKeyChecking=no -i /home/ec2-user/.ssh/id_rsa ec2-user@$IP 'sudo docker rm $RUNNING_CONTAINERS'"
          else
              echo "No running containers on $IP"
          fi          
          
          # Pull the new image and run it
          ssh -o StrictHostKeyChecking=no -i ~/.ssh/id_rsa ec2-user@$JUMP_SERVER_IP "ssh -o StrictHostKeyChecking=no -i /home/ec2-user/.ssh/id_rsa ec2-user@$IP 'sudo docker pull $ECR_REGISTRY/$ECR_REPOSITORY:$CI_COMMIT_SHA && sudo docker run -d -p 80:80 $ECR_REGISTRY/$ECR_REPOSITORY:$CI_COMMIT_SHA'"
          echo "Done with Pull"

          # Display container information
          ssh -o StrictHostKeyChecking=no -i ~/.ssh/id_rsa ec2-user@$JUMP_SERVER_IP "ssh -o StrictHostKeyChecking=no -i /home/ec2-user/.ssh/id_rsa ec2-user@$IP 'sudo docker ps -a'"
          echo "Run App"

          # Display the logs of the last started container
          ssh -o StrictHostKeyChecking=no -i ~/.ssh/id_rsa ec2-user@$JUMP_SERVER_IP "ssh -o StrictHostKeyChecking=no -i /home/ec2-user/.ssh/id_rsa ec2-user@$IP 'sudo docker logs \$(sudo docker ps -lq)'"
      done
      
      echo "Done with Pipelines"

      # Cleanup the temporary key files
      rm ~/.ssh/id_rsa ~/.ssh/id_rsa.pub

      DNS_NAME=$(aws ssm get-parameter --name "DNS" --output text --query "Parameter.Value")
      echo "DNS Name: $DNS_NAME"